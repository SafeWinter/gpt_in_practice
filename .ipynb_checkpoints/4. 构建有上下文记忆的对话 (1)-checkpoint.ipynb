{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f310313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = \"gpt4\"\n",
    "model = \"gpt-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085e53d",
   "metadata": {},
   "source": [
    "# 回顾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c34142",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m      4\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mBetween >>> and <<< are the raw search result text from google.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mExtract the answer to the question \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{query}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or say \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot found\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m if the information is not contained.\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mUse the format\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mExtracted:<answer or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot found\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m>>> \u001b[39m\u001b[38;5;132;01m{requests_result}\u001b[39;00m\u001b[38;5;124m <<<\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mExtracted:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     12\u001b[0m   input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests_result\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     13\u001b[0m   template\u001b[38;5;241m=\u001b[39mtemplate,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 17\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mquestion\u001b[49m,\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://www.baidu.com/s?wd=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m question\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     20\u001b[0m query_chain \u001b[38;5;241m=\u001b[39m LLMRequestsChain(llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mPROMPT), output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'question' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMRequestsChain\n",
    "\n",
    "\n",
    "template = \"\"\"Between >>> and <<< are the raw search result text from google.\n",
    "Extract the answer to the question '{query}' or say \"not found\" if the information is not contained.\n",
    "Use the format\n",
    "Extracted:<answer or \"not found\">\n",
    ">>> {requests_result} <<<\n",
    "Extracted:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "  input_variables=[\"query\", \"requests_result\"],\n",
    "  template=template,\n",
    ")\n",
    "\n",
    "\n",
    "query_chain = LLMRequestsChain(llm_chain = LLMChain(llm=llm, prompt=PROMPT), output_key=\"query_info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10466b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "translating_prompt_template = \"\"\"\n",
    "translate \"{query_info}\" into English:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "translating_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt=PromptTemplate.from_template(translating_prompt_template),\n",
    "    output_key = \"translated\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46480862",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m AzureChatOpenAI(deployment_name \u001b[38;5;241m=\u001b[39m deployment, model_name\u001b[38;5;241m=\u001b[39mmodel, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m      2\u001b[0m overall_chain \u001b[38;5;241m=\u001b[39m SequentialChain(\n\u001b[0;32m----> 3\u001b[0m     chains\u001b[38;5;241m=\u001b[39m[\u001b[43mquery_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m, translating_chain(llm)],\n\u001b[1;32m      4\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     output_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslated\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mquery_chain\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m      4\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mBetween >>> and <<< are the raw search result text from google.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mExtract the answer to the question \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{query}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or say \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot found\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m if the information is not contained.\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mUse the format\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mExtracted:<answer or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot found\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m>>> \u001b[39m\u001b[38;5;132;01m{requests_result}\u001b[39;00m\u001b[38;5;124m <<<\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mExtracted:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     12\u001b[0m   input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests_result\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     13\u001b[0m   template\u001b[38;5;241m=\u001b[39mtemplate,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 17\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mquestion\u001b[49m,\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://www.baidu.com/s?wd=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m question\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     20\u001b[0m requests_chain \u001b[38;5;241m=\u001b[39m LLMRequestsChain(llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mPROMPT), output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m requests_chain\n",
      "\u001b[0;31mNameError\u001b[0m: name 'question' is not defined"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(deployment_name = deployment, model_name=model, temperature=0, max_tokens=200)\n",
    "\n",
    "inputs = {\n",
    "  \"query\": question,\n",
    "  \"url\": \"http://www.baidu.com/s?wd=\" + question.replace(\" \", \"+\")\n",
    "}\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[query_chain(llm), translating_chain(llm)],\n",
    "    input_variables=[\"query\",url],\n",
    "    output_variables=[\"translated\"],\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0c4553",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'overall_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moverall_chain\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023第二季中国经济情况如何？\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'overall_chain' is not defined"
     ]
    }
   ],
   "source": [
    "overall_chain(\"2023第二季中国经济情况如何？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54124440",
   "metadata": {},
   "source": [
    "# 没有记忆的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6df8a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def get_response(input):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=deployment, # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},   \n",
    "            {\"role\": \"user\", \"content\": input}\n",
    "        ],\n",
    "        temperature = 0.9, \n",
    "        max_tokens = 200\n",
    "      )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0804701b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，非常棒，定期锻炼对健康非常有益。作为一台AI机器人，我没有实际的\"一天\"，我的主要任务就是全天候为用户提供服务和帮助。\n",
      "对不起，作为一个AI，我无法访问个人的具体日程或习惯。我只能根据你给我的信息来回答问题。如果你想要根据你的日程来制定健身计划，我可以帮助你。\n"
     ]
    }
   ],
   "source": [
    "print(get_response(\"你好，今天是周二我要去健身，我一般每周二健身。你今天干什么？\"))\n",
    "print(get_response(\"我一般周几健身？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d9a1d0",
   "metadata": {},
   "source": [
    "# 通过Gradio快速展示功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af13c666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gradio in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (3.39.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: aiohttp~=3.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (3.8.4)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (5.0.1)\n",
      "Requirement already satisfied: fastapi in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (0.95.1)\n",
      "Requirement already satisfied: ffmpy in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: gradio-client>=0.3.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: httpx in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (0.23.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (0.14.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (2.1.2)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (3.7.2)\n",
      "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (0.3.3)\n",
      "Requirement already satisfied: numpy~=1.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (1.23.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (3.9.4)\n",
      "Requirement already satisfied: packaging in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (9.5.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (1.10.7)\n",
      "Requirement already satisfied: pydub in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: requests~=2.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (2.30.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (4.5.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (0.22.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio) (11.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from aiohttp~=3.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from aiohttp~=3.0->gradio) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from aiohttp~=3.0->gradio) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from aiohttp~=3.0->gradio) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from aiohttp~=3.0->gradio) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from aiohttp~=3.0->gradio) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from aiohttp~=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: fsspec in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from gradio-client>=0.3.0->gradio) (2023.5.0)\n",
      "Requirement already satisfied: filelock in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.14.0->gradio) (3.12.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (4.41.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (6.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from requests~=2.0->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from requests~=2.0->gradio) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from requests~=2.0->gradio) (2023.5.7)\n",
      "Requirement already satisfied: click>=7.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.27.0,>=0.26.1 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from fastapi->gradio) (0.26.1)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from httpx->gradio) (0.16.3)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from httpx->gradio) (1.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from httpx->gradio) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from httpcore<0.17.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib~=3.0->gradio) (3.15.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.19.3)\n",
      "Requirement already satisfied: uc-micro-py in /Users/mbj0593/Library/Python/3.9/lib/python/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13092b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 8080\n",
      "Running on local URL:  http://127.0.0.1:7878\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "def respond(message, chat_history):\n",
    "        bot_message = get_response(message)\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "gr.close_all()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d6f45",
   "metadata": {},
   "source": [
    "# 提供外部记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d525ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def get_response(msg):\n",
    "    # print(msg)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=deployment, # engine = \"deployment_name\".\n",
    "        messages=msg,\n",
    "        temperature = 0.9, \n",
    "        max_tokens = 600\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74ee0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_to_prompt(chat_history):\n",
    "    msg = [{\"role\": \"system\", \"content\": \"You are an AI assistant.\"}]\n",
    "    i = 0\n",
    "    for round_trip in chat_history:\n",
    "        msg.append({\"role\": \"user\", \"content\": round_trip[0]})\n",
    "        msg.append({\"role\": \"assistant\", \"content\": round_trip[1]})\n",
    "    return msg\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    his_msg = history_to_prompt(chat_history)\n",
    "    his_msg.append({\"role\": \"user\", \"content\": message})\n",
    "    bot_message = get_response(his_msg)\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d60dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "gr.close_all()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d5452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
